- Vision: Scraper acadêmico-evolutivo para Shopee com foco em: conformidade lega
l (sem PII, respeitar robots.txt/ToS), robustez anti-bot, escalabilidade e manut
enção contínua.

- Goals:
  - Coletar dados públicos (busca, produto, loja, reviews) com sessão autenticad
a.
  - Persistir sessão para evitar OTP recorrente.
  - Minimizar bloqueios com throttling, delays, proxies e perfis isolados.
  - Garantir reprodutibilidade (config por .env, CLI única, logs, métricas).

- Stack:
  - Python 3.10+, Playwright (Chromium), python-dotenv, pydantic (config), tenac
ity (retries), loguru (logging), pandas (CSV).
  - Opcional (fases avançadas): Kameleo (anti-detect), 2Captcha/Anti-Captcha, SM
S API (OnlineSim/Grizzly), banco (SQLite/Postgres).

- Fase 1 — MVP mínimo (headful, sessão persistida)
  - Login manual via Playwright (headful), salvar storage_state.json.
  - Scraping de busca: keyword → lista de cards (título, preço, vendidos, URL).
  - Export: JSON e CSV simples.
  - Controles: wait_for_selector, wait_until=networkidle, delays aleatórios, tim
eouts generosos.
  - CLI: cli.py com comandos login e search.

- Fase 2 — Produto e estrutura de dados
  - Scraping de página de produto: título, preço, variações, estoque, vendedor, 
rating, imagens.
  - Schema Pydantic para validar/normalizar saídas.
  - Deduplicação por (shopid, itemid) quando disponível na URL/DOM/API interna.
  - Paginação/scroll controlado em busca/categoria.

- Fase 3 — Resiliência e detecção de bloqueios
  - Retries com backoff exponencial (tenacity) para 429/5xx.
  - Sinais de anti-bot: redirecionamento para login, CAPTCHA, layout vazio → mar
car sessão “degradada”.
  - Throttling: ~1–2 req/s por perfil; budget por minuto.
  - Health-check da sessão: teste rápido antes de lote.

- Fase 4 — Sessões, perfis e proxies
  - Vários perfis Playwright (contextos) com storage_state separados.
  - 1 IP (proxy) por perfil; IP geolocalizado ao domínio alvo (e.g., BR → shopee
.com.br).
  - Rotação de IPs controlada (não trocar IP no meio da sessão).
  - Mapeamento domínio ↔ região/IP.

- Fase 5 — Solução de CAPTCHA e OTP
  - Integração com 2Captcha/Anti-Captcha (fallback manual).
  - SMS API para registro/autenticação quando necessário.
  - Reuso agressivo de sessões para reduzir custo de OTP/CAPTCHA.

- Fase 6 — Escala e orquestração
  - Scheduler/queue (processo simples em Python ou Celery/RQ + Redis).
  - Concurrency segura: limites por perfil/conta/IP.
  - Métricas: taxa de sucesso, páginas/hora, bans/hora, latência média, retriabl
e vs fatal errors.
  - Logs estruturados (JSON) + níveis (info, warn, error).

- Fase 7 — Anti-detect (opcional)
  - Kameleo + Playwright: fingerprint realista, timezone/locale alinhados, perfi
s persistentes.
  - Gestão de perfis via API local; reaproveitamento sem exportar cookies manual
mente.

- Fase 8 — Persistência e entrega de dados
  - Arquivos: JSON/CSV por lote.
  - Banco: SQLite (local) ou Postgres (produção), upsert por chave (shopid,itemi
d).
  - Índices para consultas por categoria/keyword/data de coleta.

- Fase 9 — Observabilidade e manutenção
  - Painel simples (CLI ou notebook) para métricas e inspeção de amostras.
  - Alertas de quebra de seletor: smoke tests diários.
  - Rotina de atualização de seletores (feature flags/config por .env).

- Fase 10 — Compliance e segurança
  - Sem PII, respeitar robots.txt/ToS.
  - Rate limits conservadores; honrar disjuntores (circuit breakers) ao detectar
 bloqueios.
  - Segredos em .env/.envrc; nunca commitar storage_state.json.

- Layout de diretórios (detalhado)
  - cli.py: comandos login/search/product/export.
  - src/shopee_scraper/config.py: leitura de .env (domínio, proxy, headless, lim
ites).
  - src/shopee_scraper/session.py: criar contextos; salvar/carregar storage_stat
e; health-check.
  - src/shopee_scraper/search.py: busca, paginação/scroll, extração de cards.
  - src/shopee_scraper/product.py: parse detalhado do produto.
  - src/shopee_scraper/utils.py: delays aleatórios, parse helpers, export JSON/C
SV.
  - data/: saída de arquivos (gitignored).
  - docs/ARCHITECTURE_PLAN.txt: este plano.

- Padrões e práticas
  - Retries somente em erros transitórios; parar em login wall/CAPTCHA.
  - “Fail-fast” em sessões comprometidas; reciclar perfil/IP.
  - Testes rápidos de seletores em amostras pequenas antes de rodar lotes grande
s.

- Roadmap resumido
  1) MVP login + search. 2) Produto + schema. 3) Resiliência (retries/limites). 
4) Perfis + proxies. 5) CAPTCHA/OTP. 6) Escala/queue + métricas. 7) Anti-detect 
opcional. 8) Banco/exports. 9) Observabilidade. 10) Manutenção contínua.