Shopee Scraper — Plano de Arquitetura (2025)

Visão
- Projeto acadêmico-evolutivo para coletar dados públicos da Shopee com foco em conformidade (sem PII, respeitar robots.txt/ToS), robustez anti-bot e manutenção contínua.

Objetivos
- Coletar dados públicos (busca, produto, loja, reviews) sob sessão autenticada.
- Persistir sessão para minimizar OTP/CAPTCHA recorrentes.
- Reduzir bloqueios com throttling, delays, proxies e perfis isolados.
- Reprodutibilidade: configuração via .env, CLI única, logs e métricas.

Stack Técnica
- Python 3.10+; Playwright (para automação básica da UI) e Chrome via CDP para interceptação de rede.
- python-dotenv (configs), pydantic (Settings/Schema), tenacity (retries), loguru (logging), pandas (CSV), rich/typer (CLI UX).
- CDP client (pychrome/pycdp) para capturar chamadas de API de forma mais resiliente.
- Futuro: Kameleo (anti-detect), 2Captcha/Anti-Captcha (CAPTCHA), OnlineSim/Grizzly (OTP), SQLite/Postgres (persistência).

Fase 1 — MVP mínimo (headful, sessão persistida)
- Login manual via Playwright (headful) e gravação de storage_state.json.
- Busca inicial e extração básica de cards (título, preço, vendidos, URL) — consciente de limitações anti-bot.
- Export JSON/CSV simples em data/.
- Controles: wait_for_selector, wait_until=networkidle, delays aleatórios, timeouts generosos.
- CLI: comandos login e search.

Fase 2 — Produto e modelo de dados
- Página de produto: título, preço, variações, estoque, vendedor, rating, imagens.
- Schemas Pydantic para validação/normalização.
- Deduplicação por (shopid, itemid) sempre que disponível.
- Paginação/scroll controlado em busca/categoria.

Fase 3 — Resiliência e sinais de bloqueio
- Retries com backoff exponencial (429/5xx) usando tenacity.
- Detecção de login wall/CAPTCHA/layout vazio → marcar sessão como degradada.
- Throttling: ~1–2 req/s por perfil, orçamentos por minuto.
- Health-check de sessão antes de rodar lotes.
- Comportamento humano (mouse/scroll/typing/dwell) e consistência de locale/timezone/UA.

Fase 4 — Sessões, perfis e proxies
- Vários perfis isolados (Chrome profile por instância/conta).
- 1 IP (proxy resid./mobile) por perfil; geolocalização coerente com o domínio (ex.: BR → shopee.com.br).
- Rotação de IPs controlada (evitar troca durante a sessão; reciclar entre lotes).
- Mapeamento domínio ↔ região/IP; sticky sessions quando necessário.

Fase 5 — CAPTCHA e OTP
- Integração com 2Captcha/Anti-Captcha (fallback manual).
- SMS API para registro/autenticação quando necessário.
- Reuso agressivo de sessões para reduzir custos de OTP/CAPTCHA.

Fase 6 — Escala e orquestração
- Scheduler/queue (Celery/RQ + Redis ou pipeline simples em Python).
- Concurrency segura: limites por perfil/conta/IP.
- Métricas: taxa de sucesso, páginas/hora, bans/hora, latência média, erros transitórios vs fatais.
- Logs estruturados (JSON) com níveis (info, warn, error).

Fase 6 — Interceptação por CDP (abordagem prioritária)
- Lançar Chrome real com --remote-debugging-port=9222 usando perfil persistente e proxy.
- Conectar via CDP (pychrome/pycdp); habilitar Network domain; coletar requestWillBeSent/responseReceived/loadingFinished.
- Filtrar endpoints relevantes (ex.: /api/v4/pdp/get_pc) e obter corpo com Network.getResponseBody.
- Navegar via UI humana (home → categoria → PDP) e registrar metadados (timing, URL, headers). Evitar injeção de JS detectável.
- Alinhar Accept-Language/timezone/UA; garantir cookies/3P cookies e consent.

Fase 7 — Escala e orquestração (CDP)
- Múltiplas instâncias de Chrome isoladas (perfil, proxy, sessão) em paralelo.
- Reciclar instâncias após N páginas (50–100) para evitar acúmulo de padrões.
- Scheduler/queue (Celery/RQ) para distribuir URLs entre instâncias/locais.
- Métricas: taxa de sucesso, bans/hora, latência; logs estruturados.

Fase 8 — Alternativas mobile (fallback estratégico)
- Interceptação da API do app nativo (reverso de protocolos; extração de endpoints e tokens móveis).
- Emulador Android (Genymotion) + Chrome mobile logado + captura via ADB (system-level).
- Escolher e manter pelo menos um trilho mobile em paralelo ao CDP para resiliência de longo prazo.

Fase 9 — Anti-detect (opcional)
- Kameleo/anti-detect + Chrome/Playwright/CDP: fingerprint realista, timezone/locale alinhados, perfis persistentes.
- Gestão de perfis via API local; reaproveitamento sem exportar cookies manualmente.

Fase 10 — Persistência e entrega de dados
- Arquivos: JSON/CSV por lote em data/.
- Banco: SQLite (local) ou Postgres; upsert por (shopid, itemid).
- Índices para consultas por categoria/keyword/data.

Fase 11 — Observabilidade e manutenção
- Painel simples (CLI/Notebook) para métricas e amostras.
- Alertas de quebra de seletor; smoke tests diários.
- Rotina de atualização de seletores via config/feature flags.

Fase 12 — Compliance e segurança
- Sem PII; respeitar robots.txt e ToS.
- Rate limits conservadores; disjuntores (circuit breakers) ao detectar bloqueios.
- Segredos em .env; nunca versionar storage_state.json.

Layout de Diretórios
.
├── cli.py                  # Comandos login/search/product/export
├── requirements.txt        # Dependências do Python
├── .env.example            # Exemplo de variáveis de ambiente
├── src/
│   └── shopee_scraper/
│       ├── __init__.py
│       ├── config.py       # Settings via .env (domínio, proxy, headless, limites)
│       ├── session.py      # Criação de contextos; salvar/carregar storage_state
│       ├── search.py       # Busca, paginação/scroll, extração de cards
│       └── utils.py        # Delays, export JSON/CSV, utilidades
├── data/
│   └── .gitkeep            # Saídas (gitignored)
└── docs/
    └── ARCHITECTURE_PLAN.txt

Padrões e Boas Práticas
- Retries apenas em erros transitórios; parar em login wall/CAPTCHA.
- Fail-fast em sessões comprometidas; reciclar perfil/IP e fechar Chrome após N páginas.
- Testes rápidos de seletores/endpoint filters em amostras pequenas antes de rodadas grandes.
- Alinhar Accept-Language/locale/timezone/UA; evitar mudanças de IP no meio da sessão.
- Evitar injeções JS detectáveis; preferir observação passiva via CDP.

Roadmap Resumido
1) MVP login + search (baseline)
2) Produto + schema
3) Resiliência (retries/limites + humanização)
4) Perfis + proxies (resid./mobile)
5) CAPTCHA/OTP
6) CDP interception (coleta a partir de API de PDP)
7) Escala CDP (multi-Instância, reciclagem)
8) Trilhos mobile (app API e emulador) em paralelo
9) Anti-detect (opcional)
10) Banco/exports
11) Observabilidade
12) Manutenção contínua

-------------------------------------------------------------------------------
ADENDO (2025-08) — Estado Atual, Lacunas para Escala e Próximos Passos
-------------------------------------------------------------------------------

Estado Atual — Resumo (MVP+ com CDP)
- Captura CDP: coleta passiva de APIs (ex.: /api/v4/pdp/get_pc) em Chrome real via DevTools.
- Perfis & Proxy: `PROFILE_NAME` isola diretórios `.user-data/profiles/<name>`; `PROXY_URL` encamina tráfego (normalização p/ `--proxy-server`).
- Coerência: `Accept-Language` e `timezone` alinhados às configs; 3P cookies habilitadas por flag.
- Concurrency: abas concorrentes em lotes PDP, com `CDP_MAX_CONCURRENCY` e `stagger` por aba.
- Reciclagem: divisão automática por `PAGES_PER_SESSION` quando `--launch`, com cooldown aleatório curto.
- Proteções: disjuntor (CAPTCHA/login/inatividade/403–429), backoff (tenacity), rate limit por minuto, logs estruturados JSONL e métricas básicas via CLI.
- Fila local: scheduler de arquivos (JSON) com `queue add-*`, `queue run`, `queue list`.
- Export: normalização com Pydantic, dedup global por `(shop_id,item_id)`; saída CSV/JSON.

Lacunas para Escala (por que seu setup atual não escala)
- Única máquina/IP/perfil: um único fingerprint/IP concentra tráfego, reduz throughput e aumenta detecção/bloqueios.
- Fila local (arquivos): não distribui tarefas entre processos/hosts; sem rate limiting global por perfil/IP.
- Limitador em processo: ao adicionar workers, estoura orçamento por IP por falta de coordenação global.
- Saídas em arquivos: difícil de agregar/deduplicar e alimentar consumidores quando há paralelismo real.
- Gerência de Chrome: alocação estática de porta única (`CDP_PORT`) e risco de colisão se houver múltiplas instâncias.

Mudanças Recomendadas para Escala (o que e por quê)
1) Registro de Perfis & Proxies
   - O que: `profiles.yaml` com `profile_name`, `proxy_url (sticky)`, `locale`, `timezone`, `rps_limit`, `cdp_port_range`.
   - Por quê: vincular 1 perfil ↔ 1 IP, com limites e parâmetros por perfil, garantindo higiene geo/idioma e isolamento.

2) Fila Distribuída + Workers
   - O que: substituir scheduler de arquivos por Redis (RQ/Celery) e adicionar comando `worker` por perfil.
   - Por quê: distribuir tarefas entre múltiplos processos/hosts; roteamento por domínio/região; reintentos consistentes.

3) Rate Limiting & Locks Globais
   - O que: token buckets por `profile_name` e `proxy_url` em Redis e lock de exclusão mútua por perfil.
   - Por quê: evitar floods ao escalar horizontalmente; impedir dois workers de usar o mesmo perfil simultaneamente.

4) Gestão de Chrome/CDP
   - O que: `CDP_PORT_RANGE` e alocador de porta livre; trava por `user-data-dir`; reciclagem com jitter.
   - Por quê: rodar múltiplos Chromes sem colisão de porta/perfil; reduzir padrões detectáveis em sessões longas.

5) Persistência e Idempotência
   - O que: banco (SQLite/Postgres) com upsert por `(shop_id,item_id)` e partições por data/perfil/proxy.
   - Por quê: deduplicação cross-workers, fácil consulta/entrega e reprocessamento idempotente.

6) Observabilidade
   - O que: logs centralizados (stdout → collector), métricas por perfil/proxy (sucesso, duração, bans/hora), painel simples.
   - Por quê: operar com segurança, detectar degradações, priorizar perfis/rotas mais saudáveis.

7) Segurança e Higiene
   - O que: segredos fora do repo; `storage_state.json` nunca versionado; evitar troca de IP no meio da sessão.
   - Por quê: proteger contas e reduzir riscos operacionais.

8) Empacotamento & Deploy
   - O que: container com Chrome estável e deps Python; entrypoints `worker` e `queue`; supervisão de processos.
   - Por quê: padronizar execução e habilitar escala horizontal previsível.

Como Atingir (passos práticos e impacto)
- Curto prazo (baixo risco)
  1) `profiles.yaml` + loader; validar coerência domínio↔locale↔timezone↔proxy.
  2) Alocador de porta: `CDP_PORT_RANGE` e lock por `user-data-dir`.
  3) Containerizar (Dockerfile) e parametrizar via `.env`/perfis.

- Médio prazo (habilita escala real)
  4) Redis + RQ/Celery: trocar scheduler local; adicionar `cli worker --profile X`.
  5) Token bucket + locks (Redis): aplicar em todos caminhos CDP (busca/PDP).
  6) Upsert em SQLite/Postgres: exporters gravam também no banco.

- Longo prazo (observabilidade/resiliência)
  7) Métricas centralizadas e painel simples; alarmes por taxa de bans.
  8) Trilho mobile/anti-detect (fallback estratégico) se CDP degradar.

Critérios de Aceitação (exemplos)
- Dois workers em uma mesma máquina, cada um com perfil/IP distintos, rodando 8 abas cada, sem colisão de porta e sem violar RPS por IP.
- Reexecução de lote não duplica `(shop_id,item_id)` no banco; exports idempotentes.
- `metrics summary` mostra sucesso/latência/bloqueios por perfil/proxy de múltiplas execuções e hosts.
